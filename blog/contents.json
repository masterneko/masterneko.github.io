{
  "paths": [
    {
      "type": "file",
      "value": "lexer-guide.md"
    }
  ],
  "contents": [
    {
      "path": "lexer-guide.md",
      "url": "lexer-guide.html",
      "content": "# Lexical analysis\nImagine understanding a sentence by reading the individual characters alone. It would be quite difficult wouldn't it? \n\nLexical analysis or tokenising is the process of grouping characters into words or so called 'tokens' using a predetermined grammar. It is used extensively in compilers, programming languages and natural language processing to transform a meaningless sequence of characters into a sequence of tokens which the computer can work with.\n\nThis article will attempt to explain the workings of lexical analysis and how to write a lexer of your own.\n\n## EXAMPLE\nStarting with a pratical example, the \"Hello, World!\" programme in C would typically look like this before tokenising:\n\n```\n#include <stdio.h>\n\nint main()\n{\n    puts(\"Hello, World!\");\n}\n```\n\nHowever it may look like this after being tokenised by a compiler:\n\n[`#`] [`include`] [` `] [`<`] [`stdio.h`] [`>`] [`\\n`]\n\n[`\\n`]\n\n[`int`] [` `] [`main`] [`(`] [`)`] [`\\n`] [`{`] [`\\n`]\n\n[` `] [` `] [` `] [` `] [`puts`] [`(`] [`\"Hello, World!\"`] [`)`] [`;`] [`\\n`] \n\n[`}`]\n\nDo you notice that the characters have been chunked together. The programme has been converted from a sequence of characters to sequence of words or tokens (like a sentence). \n\nIt is also in this stage that a few very basic syntax errors are caught. For example if you missed the closing `\"` in the string literal `\"Hello, World!\"` then the compiler would likely give an error.\n\nNOTE:\nTo avoid confusion whitespace and newline characters in the example above have been retained. However in reality many compilers will remove these tokens as they have little value in most programming languages.\n\n## Grammar\nJust like English has grammar so do programming languages. Grammar defines the rules for tokenising a language. \n\nThere can be many different types of tokens. This is an outline of the five main types:\n\n- Punctuation (`{`, `}`, `;`, `<` , `>`, `>=`, `+`, `-`, `.`, `,`, etc)\n- Whitespace (`\\t`, `\\n`, `' '`)\n    - in some programming languages whitespace has semantic meaning (for example, Python)\n- Identifiers & keywords (`main`, `x`, `my_var`, `int`, `return`, `üòé`?)\n    - these are typically case sensitive, start with a letter followed by any combination of alphanumeric characters but may also support unicode characters and emojis using leftover characters (more on this later)\n- Numerical tokens (`42`, `3.14`, `0xF00D`)\n- String literals (`\"Hello!\"`, `\"This is \\\"a string\\\"\"`)\n\n## Writing a lexer\nA lexer is the algorithm which preforms the tokenisation step, breaking source code into tokens. It can be quite challenging to write a good lexer. There are many edge cases and these need to be handled effectively in order to produce the correct output.\n\n### The scanner\nWhen performing lexical analysis it is important to keep track of the current position in the source code and to be able to match against certain patterns. A scanner provides this convenience and will assist greatly you during parsing. It operates on a buffer or source file and will typically have the ability to:\n\n- Advance by `n` characters\n- Peek at characters without advancing\n- Skip a sequence of characters\n    - first checks whether a given string (for example \"cat\") matches the next following characters\n    - if it does the scanner advances its position in the buffer (by 3 characters in the case of \"cat\")\n- Skip a character if it is present in a character set\n    - for example a character set of \"abcdefghijklmnopqrstuvwxyz\" would match a single alphabetical letter\n- Skip a character if it is NOT present in a character set\n- Take a substring up to a certain point\n\nThese basic functionalities can be used in conjunction to parse tokens. For example this code could be used to parse numerical tokens:\n\n```c\nbool parse_number(Scanner& scanner)\n{\n    // skip over prefix and handle accordingly\n    if (scanner.skip_sequence(\"0x\"))\n    {\n        size_t n = 0;\n\n        // keeps matching and advancing until skip_char reached a char not in the list\n        while (scanner.skip_char(\"0123456789abcdefABCDEF\")) n++;\n\n        // if it didn't match at least once it should be rejected\n        if (n < 1)\n        {\n            return false;\n        }\n    }\n    else\n    {\n        size_t n = 0;\n\n        while (scanner.skip_char(\"0123456789\")) n++;\n\n        if (n < 1) return false;\n\n        if (scanner.skip_char('.'))\n        {\n            n = 0;\n\n            while (scanner.skip_char(\"0123456789\")) n++;\n\n            if (n < 1) return false;\n        }\n    }\n\n    return true; /* the token was accepted */\n}\n```\n\nThere could also be a parsing function for string literals or punctuation tokens, etc. It all builds on the same idea. Skip over the characters you are interested in and return whether or not the token was accepted. While it is possible to [write a lexer in other ways](/res/simple_lexer.cpp), using a scanner is one of the most easiest and least error-prone approaches.\n\nI maintain a scanner implementation at https://github.com/masterneko/scanner\n\n### Token declarations\nEventually you will implement quite a few token parsing functions. But in a source file with many different combinations of tokens how will a lexer know which function to call?\n\n```c\nbool parse_comment(Scanner& scanner);\nbool parse_number(Scanner& scanner);\nbool parse_string(Scanner& scanner);\nbool parse_more_than_or_equal(Scanner& scanner);\nbool parse_more_than(Scanner& scanner);\nbool parse_plus(Scanner& scanner);\nbool parse_minus(Scanner& scanner);\nbool parse_times(Scanner& scanner);\nbool parse_divide(Scanner& scanner);\nbool parse_space(Scanner& scanner);\n```\n\nA lexer would typically use a list of token declarations. In this list each token would be assigned a type ID and an associated parsing function. While it is possible to define a separate enum and an array of function pointers, a cool C/C++ trick is [X macros](https://en.wikipedia.org/wiki/X_macro). These allow you to define token declarations in a tabular format, all in one place.\n\n```c++\n#define TOKENS(X) \\\n      /* Type */       /* Parsing func */        \\\n    X(Comment,         parse_comment)            \\\n    X(Number,          parse_number)             \\\n    X(String,          parse_string)             \\\n    X(MoreThanOrEqual, parse_more_than_or_equal) \\\n    X(MoreThan,        parse_more_than)          \\\n    X(Plus,            parse_plus)               \\\n    X(Minus,           parse_minus)              \\\n    X(Times,           parse_times)              \\\n    X(Divide,          parse_divide)             \\\n    X(Space,           parse_space)\n```\n\nAdditionally you should also define some recycling functions. These will be used in the case that the lexer found leftover characters - an unknown token. The lexer will then attempt to repurpose these unknown characters by trying to find a recycler which accepts these characters. In our language any leftover characters will be turned into identifiers. This also has the side effect of allowing emojis which is cool üòé.\n\n```c++\nbool recycle_identifier(std::string_view leftovers)\n{\n    return true; /* accept regardless of the leftovers' contents */\n}\n\n#define RECYCLERS(X) \\\n      /* Type */       /* Recycling func */ \\\n    X(Identifier,     recycle_identifier)\n```\n\nYou will also need a Token class, this will store all information related to each individual token including its type and location within the source file as well as any text it contains:\n\n```c++\nclass Token\n{\npublic:\n    #define TOKENS_enum(TYPE_, PARSING_FUNC_) TYPE_,\n    #define RECYCLERS_enum(TYPE_, RECYCLE_FUNC_) TYPE_,\n    enum Type\n    {\n        TOKENS(TOKENS_enum)\n        RECYCLERS(RECYCLERS_enum)\n        Error\n    };\n\n    static std::string_view type_to_string(Type type)\n    {\n        #define TOKENS_strings(TYPE_, PARSING_FUNC_) #TYPE_,\n        #define RECYCLERS_strings(TYPE_, RECYCLE_FUNC_) #TYPE_,\n\n        const char* strings[] = {\n            TOKENS(TOKENS_strings)\n            RECYCLERS(RECYCLERS_strings)\n        };\n\n        return (size_t)type < (sizeof(strings) / sizeof(strings[0])) ? strings[(size_t)type] : \"Error\";\n    }\n\n    Type type;\n    std::string_view text;\n    size_t index;\n    size_t line, col;\n};\n```\n\nYou may have noticed that there is a token of type `Token::Error` and this is what the lexer creates when there is a partially digested or a non-recyclable token.\n\n### The lexer\nYou are 75% there. The last step is the lexer itself which will use the token declarations as defined earlier to convert the source file into tokens. This can however be one of the most difficult parts to master.\n\n\n#### *Worked example:*\n\nTake the following source code. The scanner will initially start off at the beginning where the arrow is pointing:\n```\n2 + num >= 5\n^\n```\n\nThe lexer should check what the kind of token is at this position. So it does a search `parse_comment()`: no, `parse_number()`: Yes - it found a match. The lexer forms a token containing the text it matched along with the location within the source file (location includes index, line and column number). It adds this to a list and moves on.\n\n```\n2 + num >= 5\n ^\n```\n\nToken list:\n```\nNumber(text: \"2\", index: 0, line: 1, col: 1)\n```\n\nFrom here the next character in the sequence is a space character. `parse_comment()`: no, `parse_number()`: no, `parse_string()`: no. Eventually it finds at the bottom of the token declarations is `parse_space()`: Yes. The lexer adds the space token to the list and moves on.\n\n```\n2 + num >= 5\n  ^\n```\n\nToken list:\n```\nNumber(text: \"2\", index: 0, line: 1, col: 1)\nSpace(text: \" \", index: 1, line: 1, col: 2)\n```\n\nNote: that the lexer is not concerned about converting number tokens into integers or floats. This can be done later. The main purpose of a lexer is to group sequences of characters into categories. Type conversion is inherently context driven (eg. `float x = 42`, here `42` would depend on knownledge that `x` is a `float` to be converted properly). Deriving context and relationships between tokens is the role of a syntatical analyser not lexical analysis.\n\nThe next character is parsed using `parse_plus` and a `Plus` token is produced and the scanner moves on.\n\n```\n2 + num >= 5\n   ^\n```\n\nToken list:\n```\nNumber(text: \"2\", index: 0, line: 1, col: 1)\nSpace(text: \" \", index: 1, line: 1, col: 2)\nPlus(text: \"+\", index 2, line: 1, col: 3)\n```\n\nThe next character is parsed by `parse_space` and another `Space` token is produced. The scanner is moved forward.\n\n```\n2 + num >= 5\n    ^\n```\n\nToken list:\n```\nNumber(text: \"2\", index: 0, line: 1, col: 1)\nSpace(text: \" \", index: 1, line: 1, col: 2)\nPlus(text: \"+\", index 2, line: 1, col: 3)\nSpace(text: \" \", index: 3, line: 1, col: 4)\n```\n\nThe character 'n' is next. Does `parse_comment()` work? no, how about `parse_number()`? no, `parse_string()`? no, `parse_more_than_or_equal`: no, `parse_more_than`: no, `parse_plus`: no, `parse_minus`: no, `parse_times`: no, `parse_divide`: no, `parse_space`: no. Oh it seems like this character has been rejected by all the parsing functions. We'll come back to this character later meanwhile we'll see if the next character can be parsed.\n\n```\n2 + num >= 5\n    *^\n```\n\n'u' also got rejected. We'll mark it with another asterisk and come back to this character once we find a token that can be parsed. \n\n```\n2 + num >= 5\n    **^\n```\n\n'm' doesn't seem to be a match either. Again we mark this character for later and move on to the next character.\n\n```\n2 + num >= 5\n    ***^\n```\n\nThis time we are in luck. `' '` is recognised by `parse_space`. Now the lexer can recycle those old rejected leftover characters into a token. \n\nThe one and only recycler `recycle_identifier` is called: `recycle_identifier(\"num\")` and returns true indicating its been accepted so a new identifier token is created out of these leftover characters.\n\nBoth the recycled `Identifier` and `Space` tokens are added to the token list.\n\nToken list:\n```\nNumber(text: \"2\", index: 0, line: 1, col: 1)\nSpace(text: \" \", index: 1, line: 1, col: 2)\nPlus(text: \"+\", index 2, line: 1, col: 3)\nSpace(text: \" \", index: 3, line: 1, col: 4)\nIdentifier(text: \"num\", index: 4, line: 1, col: 5)\nSpace(text: \" \", index: 7, line: 1, col: 8)\n```\n\nThe scanner proceeds and now the current character is '>'\n\n```\n2 + num >= 5\n        ^\n```\n\nNow there is actually two possible parsing functions the lexer could use: `parse_more_than` ('`>`') or `parse_more_than_or_equal` ('`>=`') since '>' is a substring of '>='. Logically `parse_more_than_or_equal` would be the sensible solution, however without understanding the concept of greediness the lexer could interpret this as separate `>` and `=` tokens. This is why lexers are greedy algorithms meaning they try to consume as many characters as possible.\n\nIn the end the lexer finds that `parse_more_than_or_equal` yields the most characters matched. So a `MoreThanOrEqual` token is created. The scanner as usual moves forward.\n\n```\n2 + num >= 5\n          ^\n```\n\nToken list:\n```\nNumber(text: \"2\", index: 0, line: 1, col: 1)\nSpace(text: \" \", index: 1, line: 1, col: 2)\nPlus(text: \"+\", index 2, line: 1, col: 3)\nSpace(text: \" \", index: 3, line: 1, col: 4)\nIdentifier(text: \"num\", index: 4, line: 1, col: 5)\nSpace(text: \" \", index: 7, line: 1, col: 8)\nMoreThanOrEqual(text: \">=\", index: 8, line: 1, col: 9)\n```\n\nBefore reaching the end of file (EOF) the lexer finds two more tokens. These are `Space` and `Number`.\n\n```\n2 + num >= 5\n            ^ EOF\n```\n\nSo the final token list is:\n```\nNumber(text: \"2\", index: 0, line: 1, col: 1)\nSpace(text: \" \", index: 1, line: 1, col: 2)\nPlus(text: \"+\", index 2, line: 1, col: 3)\nSpace(text: \" \", index: 3, line: 1, col: 4)\nIdentifier(text: \"num\", index: 4, line: 1, col: 5)\nSpace(text: \" \", index: 7, line: 1, col: 8)\nMoreThanOrEqual(text: \">=\", index: 8, line: 1, col: 9)\nSpace(text: \" \", index: 10, line: 1, col: 11)\nNumber(text: \"5\", index: 11, line: 1, col: 12)\n```\n\nThat is the basic algorithm of a lexer. If you were concatenate the text from each token, it would be the same as the starting source:\n\n```c\n  \"2\", \" \", \"+\", \" \", \"num\", \" \", \">=\", \" \", \"5\"\n\n= \"2 + num >= 5\"\n```\n\n### Lexer code\nA main part of the lexer alogrithm is determining the greediest token. As seen in the example above, greediness is an important aspect to lexical analysis. It can mean the difference between the code being interpreted as separate `>` and `=` tokens or as a single `>=` token. This function will try out each parser function to determine which token consumes the highest number of characters. It will store the type of token it is in `best_type` and return the end position, the point at which the token ends.\n\n```c++\n#define TOKENS_funcs(TYPE_, PARSING_FUNC_) PARSING_FUNC_,\nstatic bool (*parser_funcs[]) (Scanner& scanner) = {\n    TOKENS(TOKENS_funcs)\n};\nstatic size_t parser_funcs_n = sizeof(parser_funcs) / sizeof(parser_funcs[0]);\n\nScanner::iterator find_greediest_token(Scanner& scanner, Token::Type& best_type)\n{\n    const Scanner::iterator best_start = scanner.current();\n    Scanner::iterator best_end = best_start;\n\n    for (size_t i = 0; i < parser_funcs_n; i++)\n    {\n        bool was_accepted = parser_funcs[i](scanner);\n\n        if (was_accepted && scanner.current() > best_end) /* if it is accepted and the greediest so far */\n        {\n            // record as the best match\n            best_type = (Token::Type)i;\n            best_end = scanner.current();\n        }\n        else if (!was_accepted && scanner.current() != best_start) /* else if it was rejected halfway through parsing */\n        {\n            // create an Error token\n            best_type = Token::Type::Error;\n            best_end = scanner.current();\n        }\n\n        // reset to the beginning\n        scanner = best_start;\n    }\n\n    return best_end;\n}\n```\n\nWe will need to create tokens at some point, so here is a utility function to do that:\n\n\n```c++\nToken create_token(Scanner::iterator start_it, Token::Type type)\n{\n    Token tok;\n\n    tok.type = type;\n    // take the text present between start_it and the current character\n    tok.text = start_it.slice();\n\n    Scanner::iterator::location loc = start_it.get_location();\n\n    tok.index = loc.index;\n    tok.line = loc.line;\n    tok.col = loc.column;\n\n    return tok;\n}\n```\n\nThe `find_greediest_token` function will not always match a token, so it is important we collect the characters that weren't consumed so they can be recycled. This function is responsible for adding a token to the list and if requred will recycle leftover characters.\n\n```c++\nvoid parse_token(Scanner& scanner, std::list<Token>& tokens)\n{\n    const Scanner::iterator leftover_start = scanner.current();\n\n    while (scanner)\n    {\n        Token::Type token_type = Token::Error;\n        const Scanner::iterator token_start = scanner.current();\n        const Scanner::iterator token_end = find_greediest_token(scanner, token_type);\n\n        if (token_start == token_end) /* if no character was consumed */\n        {\n            // advance by one character (this will become a leftover character)\n            scanner++;\n        }\n        else\n        {\n            if (leftover_start != token_start) /* if there are leftovers */\n            {\n                // recycle the token into a usable type\n                tokens.push_back(recycle_token(leftover_start));\n            }\n\n            // skip forward to the token's end\n            scanner = token_end;\n\n            // add the parsed token to the list\n            tokens.push_back(create_token(token_start, token_type));\n\n            break;\n        }\n    }\n}\n```\n\nThe `recyle_token` function is quite simple, it will call each recylcler function to check whether the token can be recycled. If not it will create an error token.\n\n```c++\nToken recycle_token(Scanner::iterator start_it)\n{\n    #define RECYCLERS_funcs(TYPE_, RECYCLE_FUNC_) RECYCLE_FUNC_,\n    static bool (*recycler_funcs[]) (std::string_view text) = {\n        RECYCLERS(RECYCLERS_funcs)\n    };\n    static size_t recycler_funcs_n = sizeof(recycler_funcs) / sizeof(recycler_funcs[0]);\n\n    Token tok = create_token(start_it, Token::Type::Error);\n\n    for (size_t i = 0; i < recycler_funcs_n; i++)\n    {\n        if (recycler_funcs[i](tok.text))\n        {\n            tok.type = (Token::Type)(parser_funcs_n + i); /* recylers located directly after parsers */\n\n            break;\n        }\n    }\n\n    return tok;\n}\n```\n\n### tokenise_text\n\nThis function puts everything together in order to produce a list of tokens.\n\n```c++\nstd::list<Token> tokenise_text(std::string_view text)\n{\n    Scanner scanner(text);\n    std::list<Token> tokens;\n\n    while (scanner)\n    {\n        parse_token(scanner, tokens);\n    }\n\n    return tokens;\n}\n```\n\n\n## Conclusion\nHopefully you now know how lexical analysis works and how to write an effective lexer. The code shown in this guide is available in full [here](/res/lexer.cpp).\n\n",
      "html": "<h1 id=\"lexical-analysis\">Lexical analysis <a class=\"heading-anchor-permalink\" href=\"#lexical-analysis\">#</a></h1>\n<p>Imagine understanding a sentence by reading the individual characters alone. It would be quite difficult wouldn‚Äôt it?</p>\n<p>Lexical analysis or tokenising is the process of grouping characters into words or so called ‚Äòtokens‚Äô using a predetermined grammar. It is used extensively in compilers, programming languages and natural language processing to transform a meaningless sequence of characters into a sequence of tokens which the computer can work with.</p>\n<p>This article will attempt to explain the workings of lexical analysis and how to write a lexer of your own.</p>\n<h2 id=\"example\">EXAMPLE <a class=\"heading-anchor-permalink\" href=\"#example\">#</a></h2>\n<p>Starting with a pratical example, the ‚ÄúHello, World!‚Äù programme in C would typically look like this before tokenising:</p>\n<pre><code>#include &lt;stdio.h&gt;\n\nint main()\n{\n    puts(&quot;Hello, World!&quot;);\n}\n</code></pre>\n<p>However it may look like this after being tokenised by a compiler:</p>\n<p>[<code>#</code>] [<code>include</code>] [<code></code>] [<code>&lt;</code>] [<code>stdio.h</code>] [<code>&gt;</code>] [<code>\\n</code>]</p>\n<p>[<code>\\n</code>]</p>\n<p>[<code>int</code>] [<code></code>] [<code>main</code>] [<code>(</code>] [<code>)</code>] [<code>\\n</code>] [<code>{</code>] [<code>\\n</code>]</p>\n<p>[<code></code>] [<code></code>] [<code></code>] [<code></code>] [<code>puts</code>] [<code>(</code>] [<code>&quot;Hello, World!&quot;</code>] [<code>)</code>] [<code>;</code>] [<code>\\n</code>]</p>\n<p>[<code>}</code>]</p>\n<p>Do you notice that the characters have been chunked together. The programme has been converted from a sequence of characters to sequence of words or tokens (like a sentence).</p>\n<p>It is also in this stage that a few very basic syntax errors are caught. For example if you missed the closing <code>&quot;</code> in the string literal <code>&quot;Hello, World!&quot;</code> then the compiler would likely give an error.</p>\n<p>NOTE:\nTo avoid confusion whitespace and newline characters in the example above have been retained. However in reality many compilers will remove these tokens as they have little value in most programming languages.</p>\n<h2 id=\"grammar\">Grammar <a class=\"heading-anchor-permalink\" href=\"#grammar\">#</a></h2>\n<p>Just like English has grammar so do programming languages. Grammar defines the rules for tokenising a language.</p>\n<p>There can be many different types of tokens. This is an outline of the five main types:</p>\n<ul>\n<li>Punctuation (<code>{</code>, <code>}</code>, <code>;</code>, <code>&lt;</code> , <code>&gt;</code>, <code>&gt;=</code>, <code>+</code>, <code>-</code>, <code>.</code>, <code>,</code>, etc)</li>\n<li>Whitespace (<code>\\t</code>, <code>\\n</code>, <code>' '</code>)\n<ul>\n<li>in some programming languages whitespace has semantic meaning (for example, Python)</li>\n</ul>\n</li>\n<li>Identifiers &amp; keywords (<code>main</code>, <code>x</code>, <code>my_var</code>, <code>int</code>, <code>return</code>, <code>üòé</code>?)\n<ul>\n<li>these are typically case sensitive, start with a letter followed by any combination of alphanumeric characters but may also support unicode characters and emojis using leftover characters (more on this later)</li>\n</ul>\n</li>\n<li>Numerical tokens (<code>42</code>, <code>3.14</code>, <code>0xF00D</code>)</li>\n<li>String literals (<code>&quot;Hello!&quot;</code>, <code>&quot;This is \\&quot;a string\\&quot;&quot;</code>)</li>\n</ul>\n<h2 id=\"writing-a-lexer\">Writing a lexer <a class=\"heading-anchor-permalink\" href=\"#writing-a-lexer\">#</a></h2>\n<p>A lexer is the algorithm which preforms the tokenisation step, breaking source code into tokens. It can be quite challenging to write a good lexer. There are many edge cases and these need to be handled effectively in order to produce the correct output.</p>\n<h3 id=\"the-scanner\">The scanner <a class=\"heading-anchor-permalink\" href=\"#the-scanner\">#</a></h3>\n<p>When performing lexical analysis it is important to keep track of the current position in the source code and to be able to match against certain patterns. A scanner provides this convenience and will assist greatly you during parsing. It operates on a buffer or source file and will typically have the ability to:</p>\n<ul>\n<li>Advance by <code>n</code> characters</li>\n<li>Peek at characters without advancing</li>\n<li>Skip a sequence of characters\n<ul>\n<li>first checks whether a given string (for example ‚Äúcat‚Äù) matches the next following characters</li>\n<li>if it does the scanner advances its position in the buffer (by 3 characters in the case of ‚Äúcat‚Äù)</li>\n</ul>\n</li>\n<li>Skip a character if it is present in a character set\n<ul>\n<li>for example a character set of ‚Äúabcdefghijklmnopqrstuvwxyz‚Äù would match a single alphabetical letter</li>\n</ul>\n</li>\n<li>Skip a character if it is NOT present in a character set</li>\n<li>Take a substring up to a certain point</li>\n</ul>\n<p>These basic functionalities can be used in conjunction to parse tokens. For example this code could be used to parse numerical tokens:</p>\n<pre><code class=\"language-c\">bool parse_number(Scanner&amp; scanner)\n{\n    // skip over prefix and handle accordingly\n    if (scanner.skip_sequence(&quot;0x&quot;))\n    {\n        size_t n = 0;\n\n        // keeps matching and advancing until skip_char reached a char not in the list\n        while (scanner.skip_char(&quot;0123456789abcdefABCDEF&quot;)) n++;\n\n        // if it didn't match at least once it should be rejected\n        if (n &lt; 1)\n        {\n            return false;\n        }\n    }\n    else\n    {\n        size_t n = 0;\n\n        while (scanner.skip_char(&quot;0123456789&quot;)) n++;\n\n        if (n &lt; 1) return false;\n\n        if (scanner.skip_char('.'))\n        {\n            n = 0;\n\n            while (scanner.skip_char(&quot;0123456789&quot;)) n++;\n\n            if (n &lt; 1) return false;\n        }\n    }\n\n    return true; /* the token was accepted */\n}\n</code></pre>\n<p>There could also be a parsing function for string literals or punctuation tokens, etc. It all builds on the same idea. Skip over the characters you are interested in and return whether or not the token was accepted. While it is possible to <a href=\"/res/simple_lexer.cpp\">write a lexer in other ways</a>, using a scanner is one of the most easiest and least error-prone approaches.</p>\n<p>I maintain a scanner implementation at <a href=\"https://github.com/masterneko/scanner\">https://github.com/masterneko/scanner</a></p>\n<h3 id=\"token-declarations\">Token declarations <a class=\"heading-anchor-permalink\" href=\"#token-declarations\">#</a></h3>\n<p>Eventually you will implement quite a few token parsing functions. But in a source file with many different combinations of tokens how will a lexer know which function to call?</p>\n<pre><code class=\"language-c\">bool parse_comment(Scanner&amp; scanner);\nbool parse_number(Scanner&amp; scanner);\nbool parse_string(Scanner&amp; scanner);\nbool parse_more_than_or_equal(Scanner&amp; scanner);\nbool parse_more_than(Scanner&amp; scanner);\nbool parse_plus(Scanner&amp; scanner);\nbool parse_minus(Scanner&amp; scanner);\nbool parse_times(Scanner&amp; scanner);\nbool parse_divide(Scanner&amp; scanner);\nbool parse_space(Scanner&amp; scanner);\n</code></pre>\n<p>A lexer would typically use a list of token declarations. In this list each token would be assigned a type ID and an associated parsing function. While it is possible to define a separate enum and an array of function pointers, a cool C/C++ trick is <a href=\"https://en.wikipedia.org/wiki/X_macro\">X macros</a>. These allow you to define token declarations in a tabular format, all in one place.</p>\n<pre><code class=\"language-c++\">#define TOKENS(X) \\\n      /* Type */       /* Parsing func */        \\\n    X(Comment,         parse_comment)            \\\n    X(Number,          parse_number)             \\\n    X(String,          parse_string)             \\\n    X(MoreThanOrEqual, parse_more_than_or_equal) \\\n    X(MoreThan,        parse_more_than)          \\\n    X(Plus,            parse_plus)               \\\n    X(Minus,           parse_minus)              \\\n    X(Times,           parse_times)              \\\n    X(Divide,          parse_divide)             \\\n    X(Space,           parse_space)\n</code></pre>\n<p>Additionally you should also define some recycling functions. These will be used in the case that the lexer found leftover characters - an unknown token. The lexer will then attempt to repurpose these unknown characters by trying to find a recycler which accepts these characters. In our language any leftover characters will be turned into identifiers. This also has the side effect of allowing emojis which is cool üòé.</p>\n<pre><code class=\"language-c++\">bool recycle_identifier(std::string_view leftovers)\n{\n    return true; /* accept regardless of the leftovers' contents */\n}\n\n#define RECYCLERS(X) \\\n      /* Type */       /* Recycling func */ \\\n    X(Identifier,     recycle_identifier)\n</code></pre>\n<p>You will also need a Token class, this will store all information related to each individual token including its type and location within the source file as well as any text it contains:</p>\n<pre><code class=\"language-c++\">class Token\n{\npublic:\n    #define TOKENS_enum(TYPE_, PARSING_FUNC_) TYPE_,\n    #define RECYCLERS_enum(TYPE_, RECYCLE_FUNC_) TYPE_,\n    enum Type\n    {\n        TOKENS(TOKENS_enum)\n        RECYCLERS(RECYCLERS_enum)\n        Error\n    };\n\n    static std::string_view type_to_string(Type type)\n    {\n        #define TOKENS_strings(TYPE_, PARSING_FUNC_) #TYPE_,\n        #define RECYCLERS_strings(TYPE_, RECYCLE_FUNC_) #TYPE_,\n\n        const char* strings[] = {\n            TOKENS(TOKENS_strings)\n            RECYCLERS(RECYCLERS_strings)\n        };\n\n        return (size_t)type &lt; (sizeof(strings) / sizeof(strings[0])) ? strings[(size_t)type] : &quot;Error&quot;;\n    }\n\n    Type type;\n    std::string_view text;\n    size_t index;\n    size_t line, col;\n};\n</code></pre>\n<p>You may have noticed that there is a token of type <code>Token::Error</code> and this is what the lexer creates when there is a partially digested or a non-recyclable token.</p>\n<h3 id=\"the-lexer\">The lexer <a class=\"heading-anchor-permalink\" href=\"#the-lexer\">#</a></h3>\n<p>You are 75% there. The last step is the lexer itself which will use the token declarations as defined earlier to convert the source file into tokens. This can however be one of the most difficult parts to master.</p>\n<h4 id=\"worked-example%3A\"><em>Worked example:</em> <a class=\"heading-anchor-permalink\" href=\"#worked-example%3A\">#</a></h4>\n<p>Take the following source code. The scanner will initially start off at the beginning where the arrow is pointing:</p>\n<pre><code>2 + num &gt;= 5\n^\n</code></pre>\n<p>The lexer should check what the kind of token is at this position. So it does a search <code>parse_comment()</code>: no, <code>parse_number()</code>: Yes - it found a match. The lexer forms a token containing the text it matched along with the location within the source file (location includes index, line and column number). It adds this to a list and moves on.</p>\n<pre><code>2 + num &gt;= 5\n ^\n</code></pre>\n<p>Token list:</p>\n<pre><code>Number(text: &quot;2&quot;, index: 0, line: 1, col: 1)\n</code></pre>\n<p>From here the next character in the sequence is a space character. <code>parse_comment()</code>: no, <code>parse_number()</code>: no, <code>parse_string()</code>: no. Eventually it finds at the bottom of the token declarations is <code>parse_space()</code>: Yes. The lexer adds the space token to the list and moves on.</p>\n<pre><code>2 + num &gt;= 5\n  ^\n</code></pre>\n<p>Token list:</p>\n<pre><code>Number(text: &quot;2&quot;, index: 0, line: 1, col: 1)\nSpace(text: &quot; &quot;, index: 1, line: 1, col: 2)\n</code></pre>\n<p>Note: that the lexer is not concerned about converting number tokens into integers or floats. This can be done later. The main purpose of a lexer is to group sequences of characters into categories. Type conversion is inherently context driven (eg. <code>float x = 42</code>, here <code>42</code> would depend on knownledge that <code>x</code> is a <code>float</code> to be converted properly). Deriving context and relationships between tokens is the role of a syntatical analyser not lexical analysis.</p>\n<p>The next character is parsed using <code>parse_plus</code> and a <code>Plus</code> token is produced and the scanner moves on.</p>\n<pre><code>2 + num &gt;= 5\n   ^\n</code></pre>\n<p>Token list:</p>\n<pre><code>Number(text: &quot;2&quot;, index: 0, line: 1, col: 1)\nSpace(text: &quot; &quot;, index: 1, line: 1, col: 2)\nPlus(text: &quot;+&quot;, index 2, line: 1, col: 3)\n</code></pre>\n<p>The next character is parsed by <code>parse_space</code> and another <code>Space</code> token is produced. The scanner is moved forward.</p>\n<pre><code>2 + num &gt;= 5\n    ^\n</code></pre>\n<p>Token list:</p>\n<pre><code>Number(text: &quot;2&quot;, index: 0, line: 1, col: 1)\nSpace(text: &quot; &quot;, index: 1, line: 1, col: 2)\nPlus(text: &quot;+&quot;, index 2, line: 1, col: 3)\nSpace(text: &quot; &quot;, index: 3, line: 1, col: 4)\n</code></pre>\n<p>The character ‚Äòn‚Äô is next. Does <code>parse_comment()</code> work? no, how about <code>parse_number()</code>? no, <code>parse_string()</code>? no, <code>parse_more_than_or_equal</code>: no, <code>parse_more_than</code>: no, <code>parse_plus</code>: no, <code>parse_minus</code>: no, <code>parse_times</code>: no, <code>parse_divide</code>: no, <code>parse_space</code>: no. Oh it seems like this character has been rejected by all the parsing functions. We‚Äôll come back to this character later meanwhile we‚Äôll see if the next character can be parsed.</p>\n<pre><code>2 + num &gt;= 5\n    *^\n</code></pre>\n<p>‚Äòu‚Äô also got rejected. We‚Äôll mark it with another asterisk and come back to this character once we find a token that can be parsed.</p>\n<pre><code>2 + num &gt;= 5\n    **^\n</code></pre>\n<p>‚Äòm‚Äô doesn‚Äôt seem to be a match either. Again we mark this character for later and move on to the next character.</p>\n<pre><code>2 + num &gt;= 5\n    ***^\n</code></pre>\n<p>This time we are in luck. <code>' '</code> is recognised by <code>parse_space</code>. Now the lexer can recycle those old rejected leftover characters into a token.</p>\n<p>The one and only recycler <code>recycle_identifier</code> is called: <code>recycle_identifier(&quot;num&quot;)</code> and returns true indicating its been accepted so a new identifier token is created out of these leftover characters.</p>\n<p>Both the recycled <code>Identifier</code> and <code>Space</code> tokens are added to the token list.</p>\n<p>Token list:</p>\n<pre><code>Number(text: &quot;2&quot;, index: 0, line: 1, col: 1)\nSpace(text: &quot; &quot;, index: 1, line: 1, col: 2)\nPlus(text: &quot;+&quot;, index 2, line: 1, col: 3)\nSpace(text: &quot; &quot;, index: 3, line: 1, col: 4)\nIdentifier(text: &quot;num&quot;, index: 4, line: 1, col: 5)\nSpace(text: &quot; &quot;, index: 7, line: 1, col: 8)\n</code></pre>\n<p>The scanner proceeds and now the current character is ‚Äò&gt;‚Äô</p>\n<pre><code>2 + num &gt;= 5\n        ^\n</code></pre>\n<p>Now there is actually two possible parsing functions the lexer could use: <code>parse_more_than</code> (‚Äô<code>&gt;</code>‚Äô) or <code>parse_more_than_or_equal</code> (‚Äô<code>&gt;=</code>‚Äô) since ‚Äò&gt;‚Äô is a substring of ‚Äò&gt;=‚Äô. Logically <code>parse_more_than_or_equal</code> would be the sensible solution, however without understanding the concept of greediness the lexer could interpret this as separate <code>&gt;</code> and <code>=</code> tokens. This is why lexers are greedy algorithms meaning they try to consume as many characters as possible.</p>\n<p>In the end the lexer finds that <code>parse_more_than_or_equal</code> yields the most characters matched. So a <code>MoreThanOrEqual</code> token is created. The scanner as usual moves forward.</p>\n<pre><code>2 + num &gt;= 5\n          ^\n</code></pre>\n<p>Token list:</p>\n<pre><code>Number(text: &quot;2&quot;, index: 0, line: 1, col: 1)\nSpace(text: &quot; &quot;, index: 1, line: 1, col: 2)\nPlus(text: &quot;+&quot;, index 2, line: 1, col: 3)\nSpace(text: &quot; &quot;, index: 3, line: 1, col: 4)\nIdentifier(text: &quot;num&quot;, index: 4, line: 1, col: 5)\nSpace(text: &quot; &quot;, index: 7, line: 1, col: 8)\nMoreThanOrEqual(text: &quot;&gt;=&quot;, index: 8, line: 1, col: 9)\n</code></pre>\n<p>Before reaching the end of file (EOF) the lexer finds two more tokens. These are <code>Space</code> and <code>Number</code>.</p>\n<pre><code>2 + num &gt;= 5\n            ^ EOF\n</code></pre>\n<p>So the final token list is:</p>\n<pre><code>Number(text: &quot;2&quot;, index: 0, line: 1, col: 1)\nSpace(text: &quot; &quot;, index: 1, line: 1, col: 2)\nPlus(text: &quot;+&quot;, index 2, line: 1, col: 3)\nSpace(text: &quot; &quot;, index: 3, line: 1, col: 4)\nIdentifier(text: &quot;num&quot;, index: 4, line: 1, col: 5)\nSpace(text: &quot; &quot;, index: 7, line: 1, col: 8)\nMoreThanOrEqual(text: &quot;&gt;=&quot;, index: 8, line: 1, col: 9)\nSpace(text: &quot; &quot;, index: 10, line: 1, col: 11)\nNumber(text: &quot;5&quot;, index: 11, line: 1, col: 12)\n</code></pre>\n<p>That is the basic algorithm of a lexer. If you were concatenate the text from each token, it would be the same as the starting source:</p>\n<pre><code class=\"language-c\">  &quot;2&quot;, &quot; &quot;, &quot;+&quot;, &quot; &quot;, &quot;num&quot;, &quot; &quot;, &quot;&gt;=&quot;, &quot; &quot;, &quot;5&quot;\n\n= &quot;2 + num &gt;= 5&quot;\n</code></pre>\n<h3 id=\"lexer-code\">Lexer code <a class=\"heading-anchor-permalink\" href=\"#lexer-code\">#</a></h3>\n<p>A main part of the lexer alogrithm is determining the greediest token. As seen in the example above, greediness is an important aspect to lexical analysis. It can mean the difference between the code being interpreted as separate <code>&gt;</code> and <code>=</code> tokens or as a single <code>&gt;=</code> token. This function will try out each parser function to determine which token consumes the highest number of characters. It will store the type of token it is in <code>best_type</code> and return the end position, the point at which the token ends.</p>\n<pre><code class=\"language-c++\">#define TOKENS_funcs(TYPE_, PARSING_FUNC_) PARSING_FUNC_,\nstatic bool (*parser_funcs[]) (Scanner&amp; scanner) = {\n    TOKENS(TOKENS_funcs)\n};\nstatic size_t parser_funcs_n = sizeof(parser_funcs) / sizeof(parser_funcs[0]);\n\nScanner::iterator find_greediest_token(Scanner&amp; scanner, Token::Type&amp; best_type)\n{\n    const Scanner::iterator best_start = scanner.current();\n    Scanner::iterator best_end = best_start;\n\n    for (size_t i = 0; i &lt; parser_funcs_n; i++)\n    {\n        bool was_accepted = parser_funcs[i](scanner);\n\n        if (was_accepted &amp;&amp; scanner.current() &gt; best_end) /* if it is accepted and the greediest so far */\n        {\n            // record as the best match\n            best_type = (Token::Type)i;\n            best_end = scanner.current();\n        }\n        else if (!was_accepted &amp;&amp; scanner.current() != best_start) /* else if it was rejected halfway through parsing */\n        {\n            // create an Error token\n            best_type = Token::Type::Error;\n            best_end = scanner.current();\n        }\n\n        // reset to the beginning\n        scanner = best_start;\n    }\n\n    return best_end;\n}\n</code></pre>\n<p>We will need to create tokens at some point, so here is a utility function to do that:</p>\n<pre><code class=\"language-c++\">Token create_token(Scanner::iterator start_it, Token::Type type)\n{\n    Token tok;\n\n    tok.type = type;\n    // take the text present between start_it and the current character\n    tok.text = start_it.slice();\n\n    Scanner::iterator::location loc = start_it.get_location();\n\n    tok.index = loc.index;\n    tok.line = loc.line;\n    tok.col = loc.column;\n\n    return tok;\n}\n</code></pre>\n<p>The <code>find_greediest_token</code> function will not always match a token, so it is important we collect the characters that weren‚Äôt consumed so they can be recycled. This function is responsible for adding a token to the list and if requred will recycle leftover characters.</p>\n<pre><code class=\"language-c++\">void parse_token(Scanner&amp; scanner, std::list&lt;Token&gt;&amp; tokens)\n{\n    const Scanner::iterator leftover_start = scanner.current();\n\n    while (scanner)\n    {\n        Token::Type token_type = Token::Error;\n        const Scanner::iterator token_start = scanner.current();\n        const Scanner::iterator token_end = find_greediest_token(scanner, token_type);\n\n        if (token_start == token_end) /* if no character was consumed */\n        {\n            // advance by one character (this will become a leftover character)\n            scanner++;\n        }\n        else\n        {\n            if (leftover_start != token_start) /* if there are leftovers */\n            {\n                // recycle the token into a usable type\n                tokens.push_back(recycle_token(leftover_start));\n            }\n\n            // skip forward to the token's end\n            scanner = token_end;\n\n            // add the parsed token to the list\n            tokens.push_back(create_token(token_start, token_type));\n\n            break;\n        }\n    }\n}\n</code></pre>\n<p>The <code>recyle_token</code> function is quite simple, it will call each recylcler function to check whether the token can be recycled. If not it will create an error token.</p>\n<pre><code class=\"language-c++\">Token recycle_token(Scanner::iterator start_it)\n{\n    #define RECYCLERS_funcs(TYPE_, RECYCLE_FUNC_) RECYCLE_FUNC_,\n    static bool (*recycler_funcs[]) (std::string_view text) = {\n        RECYCLERS(RECYCLERS_funcs)\n    };\n    static size_t recycler_funcs_n = sizeof(recycler_funcs) / sizeof(recycler_funcs[0]);\n\n    Token tok = create_token(start_it, Token::Type::Error);\n\n    for (size_t i = 0; i &lt; recycler_funcs_n; i++)\n    {\n        if (recycler_funcs[i](tok.text))\n        {\n            tok.type = (Token::Type)(parser_funcs_n + i); /* recylers located directly after parsers */\n\n            break;\n        }\n    }\n\n    return tok;\n}\n</code></pre>\n<h3 id=\"tokenise_text\">tokenise_text <a class=\"heading-anchor-permalink\" href=\"#tokenise_text\">#</a></h3>\n<p>This function puts everything together in order to produce a list of tokens.</p>\n<pre><code class=\"language-c++\">std::list&lt;Token&gt; tokenise_text(std::string_view text)\n{\n    Scanner scanner(text);\n    std::list&lt;Token&gt; tokens;\n\n    while (scanner)\n    {\n        parse_token(scanner, tokens);\n    }\n\n    return tokens;\n}\n</code></pre>\n<h2 id=\"conclusion\">Conclusion <a class=\"heading-anchor-permalink\" href=\"#conclusion\">#</a></h2>\n<p>Hopefully you now know how lexical analysis works and how to write an effective lexer. The code shown in this guide is available in full <a href=\"/res/lexer.cpp\">here</a>.</p>\n",
      "id": 0
    }
  ]
}